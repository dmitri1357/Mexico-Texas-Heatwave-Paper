#!/usr/bin/env python3

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt   
from scipy import stats
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
from tqdm import tqdm

# set font to Arial
import matplotlib as mpl
mpl.rc('font',family='Arial')

# datasets can be found in accompanying Zenodo repository

all_corrs = np.load('all_corrs.npy') # load daily pattern correlations file

dates = pd.date_range(start='1/1/1940', end='12/31/2023')
dates = dates[~((dates.month == 2) & (dates.day == 29))]
dates = pd.DataFrame(dates)
dates['month'] = pd.to_datetime(dates.iloc[:,0]).dt.month
dates = dates[dates.month.isin([4,5,6,7,8,9])]
dates.reset_index(inplace=True)

day_idx = np.arange(1,184,1)
day_idxs = np.tile(day_idx,83)

# match patterns for each day during 14-day heatwave period (June 15-28, 2023)
for m in range(14):
    corrs = all_corrs[15264+m,:] # starting at day 15264 = 6/15/2023

    df1 = pd.DataFrame({'date':dates.iloc[:,1],'month':dates.month,
                        'corr':corrs})  

    df1['year'] = pd.to_datetime(df1.date).dt.year
    df2 = df1[df1.year<=2022]
    df2['day_idx'] = day_idxs
    df3 = df2[df2.day_idx.between(61+m,91+m)]

    dfx = df3.sort_values(by='corr',ascending=False)
    day_accum = []
    for k in range(df3.shape[0]):
        day = dfx.index.values[k]
        window = list(np.arange(day-3,day+4,1)) # 7-day window
        i, j = window[0], window[-1]
        res = any(ele >= i and ele <= j for ele in day_accum)
        if res == 0:
            day_accum.append(day)
    day_accum = np.array(day_accum)

    df5 = df3.loc[day_accum]
    df8 = df5.iloc[:40] # grab top-40

    mats_idx = df8.index.values

    np.save(f'mats_idx{m}',mats_idx) # save the index of matched days for each of the 14 days during heatwave

# load domain-averaged variables
z500_amjjas2_z = np.load('zvec_smaller.npy')
z500_amjjas2_z = np.nanmax(z500_amjjas2_z,axis=1) # using max of detrended Z500 z-scores in smaller rectangle as predictor

tmax_detrended_z = np.load('tmax_detrended_z.npy')
tmax_z_normed2 = np.load('tmax_z_normed2.npy')
sm_z_normed2 = np.load('sm_z_normed2.npy')

dates = pd.date_range(start='1/1/1940', end='12/31/2023')
dates = dates[~((dates.month == 2) & (dates.day == 29))]
dates = pd.DataFrame(dates)
dates['month'] = pd.to_datetime(dates.iloc[:,0]).dt.month
dates = dates[dates.month.isin([4,5,6,7,8,9])]
idx = dates.index.values

# run predictive models (simple and MLR) for analogs

actuals = []
preds = []
preds_lower = []
preds_upper = []
preds_mlr = []
preds_mlr_lower = []
preds_mlr_upper = []
for m in range(14):
    mats_idx = np.load(f'mats_idx{m}.npy') # for each of the 14 heatwave days, load the index of top-40 matched patterns (these were generated by previous code block above)
    
    zv = z500_amjjas2_z
    z = zv[mats_idx]
    tv = tmax_detrended_z # detrended tmax z-scores to train the models
    t = tv[mats_idx]
    
    day = zv[15264+m]
    day2 = tmax_z_normed2[15264+m] # actual z-scores (not detrended)
    actuals.append(day2)
    
    new_val = np.array(day)
    new_val = new_val.reshape(-1,1)
    
    X = pd.DataFrame({'z500':z})
    y = pd.DataFrame({'tmax':t})
        
    model = sm.OLS(y, X).fit()
         
    # Get the predictions and confidence intervals
    predictions = model.get_prediction(new_val)
    summary_frame = predictions.summary_frame(alpha=0.1)  # 95% confidence interval
    
    preds.append(np.float32(summary_frame['mean']))
    
    # Extract the confidence intervals
    preds_lower.append(np.float32(summary_frame['mean_ci_lower']))
    preds_upper.append(np.float32(summary_frame['mean_ci_upper']))
    
    # MLR models
    
    s = sm_z_normed2[mats_idx] # bringing in SM as 2nd predictor
    
    predict_df = pd.DataFrame({'z500':z,
                               'SM':s})
    X = predict_df
    y = pd.DataFrame({'tmax':t})

    model = sm.OLS(y, X).fit() # fitting MLR model using statsmodels package
    
    # Get the predictions and confidence intervals
    predictions = model.get_prediction(np.array([zv[15264+m],sm_z_normed2[15264+m]])) # using MLR
    summary_frame = predictions.summary_frame(alpha=0.1)  # 95% confidence interval
    
    preds_mlr.append(np.float32(summary_frame['mean']))
    
    # Extract the confidence intervals
    preds_mlr_lower.append(np.float32(summary_frame['mean_ci_lower']))
    preds_mlr_upper.append(np.float32(summary_frame['mean_ci_upper']))
    
preds = np.squeeze(np.array(preds)) # predictions of detrended Tmax z-scores
preds_lower = np.squeeze(np.array(preds_lower)) # predictions of detrended Tmax z-scores
preds_upper = np.squeeze(np.array(preds_upper)) # predictions of detrended Tmax z-scores

# first, compute the detrended Tmax values that these z-scores represent
daily_std_detrended = np.load('daily_std_detrended.npy')
std_deg_C = daily_std_detrended[15264:15278]
dep_deg_C = std_deg_C * preds
daily_avg_detrended = np.load('daily_avg_detrended.npy')
pred_deg_C = daily_avg_detrended[15264:15278] + dep_deg_C

dep_deg_C_lower = std_deg_C * preds_lower
dep_deg_C_upper = std_deg_C * preds_upper
pred_deg_C_lower = daily_avg_detrended[15264:15278] + dep_deg_C_lower
pred_deg_C_upper = daily_avg_detrended[15264:15278] + dep_deg_C_upper

# add Tmax trend back in
trend_increments = np.load('tmax_trend_increments.npy')
this_increment = trend_increments[-1]
preds_trend_added = pred_deg_C + this_increment

tmax_normed = np.load('tmax_normed.npy') 
tmax_normed_domain = np.nanmean(tmax_normed,axis=1) # observed domain-average Tmax in actual units (Deg C), latitude normalized
observed_tmax = tmax_normed_domain[15264:15278]

# tie % explained to anomalies in deg C, so they are directly interpretable from panel A
averages = daily_avg_detrended[15264:15278]
full_deps = observed_tmax - averages
perc_explained_circulation = ((pred_deg_C-averages)/full_deps)*100
perc_explained_circulation = np.where(perc_explained_circulation<0,0,perc_explained_circulation)
perc_explained_trend_added = ((preds_trend_added-averages)/full_deps)*100

perc_explained_circulation_lower = ((pred_deg_C_lower-averages)/full_deps)*100
perc_explained_circulation_lower = np.where(perc_explained_circulation_lower<0,0,perc_explained_circulation_lower)
perc_explained_circulation_upper = ((pred_deg_C_upper-averages)/full_deps)*100
perc_explained_circulation_upper = np.where(perc_explained_circulation_upper<0,0,perc_explained_circulation_upper)

# repeat the above workflow for MLR

preds_mlr = np.squeeze(np.array(preds_mlr)) # predictions of detrended Tmax z-scores
preds_mlr_lower = np.squeeze(np.array(preds_mlr_lower)) # predictions of detrended Tmax z-scores
preds_mlr_upper = np.squeeze(np.array(preds_mlr_upper)) # predictions of detrended Tmax z-scores

# first, compute the detrended Tmax values that these z-scores represent
daily_std_detrended = np.load('daily_std_detrended.npy')
std_deg_C = daily_std_detrended[15264:15278]
dep_deg_C_mlr = std_deg_C * preds_mlr
daily_avg_detrended = np.load('daily_avg_detrended.npy')
pred_deg_C_mlr = daily_avg_detrended[15264:15278] + dep_deg_C_mlr

dep_deg_C_lower_mlr = std_deg_C * preds_mlr_lower
dep_deg_C_upper_mlr = std_deg_C * preds_mlr_upper
pred_deg_C_lower_mlr = daily_avg_detrended[15264:15278] + dep_deg_C_lower_mlr
pred_deg_C_upper_mlr = daily_avg_detrended[15264:15278] + dep_deg_C_upper_mlr

# add Tmax trend back in
trend_increments = np.load('tmax_trend_increments.npy')
this_increment = trend_increments[-1]
preds_trend_added_mlr = pred_deg_C_mlr + this_increment

preds_trend_added_mlr_lower = pred_deg_C_lower_mlr + this_increment
preds_trend_added_mlr_upper = pred_deg_C_upper_mlr + this_increment

# tie % explained to anomalies in deg C, so they are directly interpretable from panel A
averages = daily_avg_detrended[15264:15278]
full_deps = observed_tmax - averages
perc_explained_circulation_mlr = ((pred_deg_C_mlr-averages)/full_deps)*100
perc_explained_circulation_mlr = np.where(perc_explained_circulation_mlr<0,0,perc_explained_circulation_mlr)
perc_explained_trend_added_mlr = ((preds_trend_added_mlr-averages)/full_deps)*100

perc_explained_circulation_mlr_lower = ((pred_deg_C_lower_mlr-averages)/full_deps)*100
perc_explained_circulation_mlr_lower = np.where(perc_explained_circulation_mlr_lower<0,0,perc_explained_circulation_mlr_lower)
perc_explained_circulation_mlr_upper = ((pred_deg_C_upper_mlr-averages)/full_deps)*100
perc_explained_circulation_mlr_upper = np.where(perc_explained_circulation_mlr_upper<0,0,perc_explained_circulation_mlr_upper)
perc_explained_trend_added_mlr_lower = ((preds_trend_added_mlr_lower-averages)/full_deps)*100
perc_explained_trend_added_mlr_upper = ((preds_trend_added_mlr_upper-averages)/full_deps)*100

# set upper bound of "perc_explained_trend_added_mlr_upper" to 100%
perc_explained_trend_added_mlr_upper = np.where(perc_explained_trend_added_mlr_upper>100,100,perc_explained_trend_added_mlr_upper)

# plot fig 3a

fig, ax1 = plt.subplots(figsize=(8,6))
ax1.set_ylabel('Domain-averaged Tmax ($^\circ$C)', fontsize = 16)
ax1.set_yticks(np.arange(30,40.01,1))
ax1.set_ylim(top=40)
ax1.set_ylim(bottom=30)
ax1.set_xticks([0,3,6,9,12])
ax1.set_xticklabels(['June 15','June 18','June 21','June 24','June 27'], fontsize = 13)
ax1.plot(observed_tmax,'-o', color='firebrick', lw=2)
ax1.plot(pred_deg_C,'-o', color='0.5', lw=2)
ax1.fill_between(np.arange(14),pred_deg_C_lower,pred_deg_C_upper,color='0.5',alpha=0.4)
ax1.plot(pred_deg_C_mlr,'-o', color='k', lw=2)
ax1.fill_between(np.arange(14),pred_deg_C_lower_mlr,pred_deg_C_upper_mlr,color='k',alpha=0.4)
ax1.plot(preds_trend_added_mlr,'-o', color='cornflowerblue', lw=2)
ax1.fill_between(np.arange(14),preds_trend_added_mlr_lower,preds_trend_added_mlr_upper,color='cornflowerblue',alpha=0.4)
ax1.tick_params(axis='y', labelsize=13)
plt.title('June 15-28, 2023 daily domain-averaged \n Tmax and analogue Tmax', fontsize = 20)
plt.savefig('fig_3c_90_CI.pdf',dpi=600)

df_3b = pd.DataFrame({'observed':observed_tmax,
                      'z500_pred':pred_deg_C,
                      'z500_lower':pred_deg_C_lower,
                      'z500_upper':pred_deg_C_upper,
                      'mlr_pred':pred_deg_C_mlr,
                      'mlr_lower':pred_deg_C_lower_mlr,
                      'mlr_upper':pred_deg_C_upper_mlr,
                      'trend_added':preds_trend_added_mlr,
                      'trend_added_lower':preds_trend_added_mlr_lower,
                      'trend_added_upper':preds_trend_added_mlr_upper})
dates = pd.date_range(start='6/15/2023', end='6/28/2023')
df_3b.set_index(dates,inplace=True)
df_3b = np.round(df_3b,2)

# re-plot this on extended y-axis to make legend
fig = plt.figure(figsize=(8,6))
plt.plot(observed_tmax,'-o', color='firebrick', lw=2)
plt.plot(preds_trend_added_mlr,'-o', color='cornflowerblue', lw=2)
plt.plot(pred_deg_C_mlr,'-o', color='k', lw=2)
plt.plot(pred_deg_C,'-o', color='0.5', lw=2)
plt.yticks(np.arange(-5000,6001,1000), fontsize=13)
l = plt.legend(['2023 Tmax',
            'Tmax predicted by Z500 + SM + Tmax trend',
            'Tmax predicted by Z500 + SM',
            'Tmax predicted by Z500 only'], 
            loc='lower right', fontsize=15)
colors = ['firebrick','cornflowerblue','k','0.5']
n = -1
for text in l.get_texts():
    n += 1
    text.set_color(colors[n])
plt.savefig('fig_3c_legend.pdf',dpi=600)

# plot fig 3b

fig, ax2 = plt.subplots(figsize=(8,6))
color = 'tab:brown'
ax2.set_ylabel('% of +Tmax anomalies explained', color='k', fontsize=16)  
ax2.set_yticks(np.arange(0,101,10))
ax2.set_ylim(top=102)
ax2.set_ylim(bottom=-2)
ax2.set_xticks([0,3,6,9,12])
ax2.set_xticklabels(['June 15','June 18','June 21','June 24','June 27'], fontsize = 13)
ax2.plot(perc_explained_circulation, '-o', color='0.5', lw=2)
ax2.fill_between(np.arange(14),perc_explained_circulation_lower,perc_explained_circulation_upper,color='0.5',alpha=0.4)
ax2.plot(perc_explained_circulation_mlr, '-o', color='k', lw=2)
ax2.fill_between(np.arange(14),perc_explained_circulation_mlr_lower,perc_explained_circulation_mlr_upper,color='k',alpha=0.4)
ax2.plot(perc_explained_trend_added_mlr, '-o', color='cornflowerblue', lw=2)
ax2.fill_between(np.arange(14),perc_explained_trend_added_mlr_lower,perc_explained_trend_added_mlr_upper,color='cornflowerblue',alpha=0.4)
ax2.tick_params(axis='y', labelcolor='k')
ax2.tick_params(axis='y', labelsize=13)
plt.title('June 15-28, 2023 percent of \n +Tmax anomalies explained by analogues', fontsize = 20)
plt.show()
plt.savefig('fig_3b_90_CI.pdf',dpi=600)

df_3c = pd.DataFrame({'z500_pred':perc_explained_circulation,
                      'z500_lower':perc_explained_circulation_lower,
                      'z500_upper':perc_explained_circulation_upper,
                      'mlr_pred':perc_explained_circulation_mlr,
                      'mlr_lower':perc_explained_circulation_mlr_lower,
                      'mlr_upper':perc_explained_circulation_mlr_upper,
                      'trend_added':perc_explained_trend_added_mlr,
                      'trend_added_lower':perc_explained_trend_added_mlr_lower,
                      'trend_added_upper':perc_explained_trend_added_mlr_upper})
dates = pd.date_range(start='6/15/2023', end='6/28/2023')
df_3c.set_index(dates,inplace=True)
df_3c = np.round(df_3c,2)

# re-plot this on extended y-axis to make legend
fig = plt.figure(figsize=(8,6))
plt.plot(perc_explained_trend_added_mlr, '-o', color='cornflowerblue', lw=2)
plt.plot(perc_explained_circulation_mlr, '-o', color='k', lw=2)
plt.plot(perc_explained_circulation, '-o', color='0.5', lw=2)
plt.yticks(np.arange(-5000,6001,1000), fontsize=13)
l = plt.legend(['% explained by Z500 + SM + Tmax trend',
                '% explained by Z500 + SM',
                '% explained by Z500 only'],
               loc='lower right', fontsize=15)
colors = ['cornflowerblue','k','0.5']
n = -1
for text in l.get_texts():
    n += 1
    text.set_color(colors[n])
plt.savefig('fig_3b_legend.pdf',dpi=600)

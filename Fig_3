#!/usr/bin/env python3

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt   
from scipy import stats
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
from tqdm import tqdm

# set font to Arial
import matplotlib as mpl
mpl.rc('font',family='Arial')

# datasets can be found in accompanying Zenodo repository

# all_corrs = np.load('all_corrs.npy')
# all_corrs = all_corrs[1:,:] # remove the 1st entry which was empty from the np.vstack operation
# np.save('all_corrs',all_corrs.astype('float32'))
all_corrs = np.load('all_corrs.npy') # load daily pattern correlations file

dates = pd.date_range(start='1/1/1940', end='12/31/2023')
dates = dates[~((dates.month == 2) & (dates.day == 29))]
dates = pd.DataFrame(dates)
dates['month'] = pd.to_datetime(dates.iloc[:,0]).dt.month
dates = dates[dates.month.isin([4,5,6,7,8,9])]
dates.reset_index(inplace=True)

day_idx = np.arange(1,184,1)
day_idxs = np.tile(day_idx,83)

# # test number of analogues between 20-100 by computing median pattern correlation
# # testing done for 6/20/2023

# median_corrs = []
# analogue_number = range(20,101)
# for num in tqdm(analogue_number):
#     corrs = all_corrs[15269,:] # day 15269 = 6/20/2023

#     df1 = pd.DataFrame({'date':dates.iloc[:,1],'month':dates.month,
#                         'corr':corrs})  

#     df1['year'] = pd.to_datetime(df1.date).dt.year
#     df2 = df1[df1.year<=2022]
#     df2['day_idx'] = day_idxs
#     df3 = df2[df2.day_idx.between(51,111)]

#     dfx = df3.sort_values(by='corr',ascending=False)
#     day_accum = []
#     for k in range(df3.shape[0]):
#         day = dfx.index.values[k]
#         window = list(np.arange(day-3,day+4,1)) # 7-day window
#         i, j = window[0], window[-1]
#         res = any(ele >= i and ele <= j for ele in day_accum)
#         if res == 0:
#             day_accum.append(day)
#     day_accum = np.array(day_accum)

#     df5 = df3.loc[day_accum]
#     df8 = df5.iloc[:num] # analogue number is changed here 

#     median_corrs.append(np.median(df8['corr']))

# # plot the corr values for each analogue size
# plt.figure(figsize=(8, 5))
# plt.plot(analogue_number, median_corrs, marker='o', linestyle='-', color='tab:red', label='Median pattern corr.')
# plt.title('Median pattern correlation by number of analogues', fontsize=14)
# plt.xlabel('Number of analogues', fontsize=12)
# plt.ylabel('Correlation coefficient', fontsize=12)
# plt.xticks(fontsize=10)
# plt.yticks(fontsize=10)
# #plt.legend(fontsize=10)
# #plt.grid(alpha=0.3)
# plt.show()

# match patterns for each day during 14-day heatwave period (June 15-28, 2023)
for m in range(14):
    corrs = all_corrs[15264+m,:] # starting at day 15264 = 6/15/2023

    df1 = pd.DataFrame({'date':dates.iloc[:,1],'month':dates.month,
                        'corr':corrs})  

    df1['year'] = pd.to_datetime(df1.date).dt.year
    df2 = df1[df1.year<=2022]
    df2['day_idx'] = day_idxs
    df3 = df2[df2.day_idx.between(46+m,106+m)]

    dfx = df3.sort_values(by='corr',ascending=False)
    day_accum = []
    for k in range(df3.shape[0]):
        day = dfx.index.values[k]
        window = list(np.arange(day-3,day+4,1)) # 7-day window
        i, j = window[0], window[-1]
        res = any(ele >= i and ele <= j for ele in day_accum)
        if res == 0:
            day_accum.append(day)
    day_accum = np.array(day_accum)

    df5 = df3.loc[day_accum]
    df8 = df5.iloc[:100] # grab top-100

    mats_idx = df8.index.values

    np.save(f'mats_idx{m}',mats_idx) # save the index of matched days for each of the 14 days during heatwave

# load domain-averaged variables
z500_amjjas2_z = np.load('zvec_smaller.npy')
z500_amjjas2_z = np.nanmax(z500_amjjas2_z,axis=1) # using max of detrended Z500 z-scores in smaller rectangle as predictor

tmax_detrended_z = np.load('tmax_detrended_z.npy')
tmax_z_normed2 = np.load('tmax_z_normed2.npy')
sm_z_normed2 = np.load('sm_z_normed2.npy')

dates = pd.date_range(start='1/1/1940', end='12/31/2023')
dates = dates[~((dates.month == 2) & (dates.day == 29))]
dates = pd.DataFrame(dates)
dates['month'] = pd.to_datetime(dates.iloc[:,0]).dt.month
dates = dates[dates.month.isin([4,5,6,7,8,9])]
idx = dates.index.values

# run predictive models (simple and MLR) for analogs
# these are done with SM lag -1 following revisions

actuals = []
preds = []
preds_lower = []
preds_upper = []
preds_mlr = []
preds_mlr_lower = []
preds_mlr_upper = []
preds_mlr2 = []
preds_mlr_lower2 = []
preds_mlr_upper2 = []
for m in range(14):
    mats_idx = np.load(f'mats_idx{m}.npy') # for each of the 14 heatwave days, load the index of top-40 matched patterns (these were generated by previous code block above)
    
    zv = z500_amjjas2_z
    z = zv[mats_idx]
    tv = tmax_detrended_z # detrended tmax z-scores to train the models
    t = tv[mats_idx]
    
    day = zv[15264+m]
    day2 = tmax_z_normed2[15264+m] # actual z-scores (not detrended)
    actuals.append(day2)
    
    new_val = np.array(day)
    new_val = new_val.reshape(-1,1)
    
    X = pd.DataFrame({'z500':z})
    y = pd.DataFrame({'tmax':t})
        
    model = sm.OLS(y, X).fit()
         
    # Get the predictions and confidence intervals
    predictions = model.get_prediction(new_val)
    summary_frame = predictions.summary_frame(alpha=0.1)  # 90% confidence interval
    
    preds.append(np.float32(summary_frame['mean']))
    
    # Extract the confidence intervals
    preds_lower.append(np.float32(summary_frame['mean_ci_lower']))
    preds_upper.append(np.float32(summary_frame['mean_ci_upper']))
    
    # MLR models - Z500 and SM
    
    # changed SM index to -1 to indicate prior-day SM
    s = sm_z_normed2[mats_idx-1] # bringing in SM as 2nd predictor
    
    predict_df = pd.DataFrame({'z500':z,
                               'SM':s})
    X = predict_df
    y = pd.DataFrame({'tmax':t})

    model = sm.OLS(y, X).fit() # fitting MLR model using statsmodels package
    
    # Get the predictions and confidence intervals
    predictions = model.get_prediction(np.array([zv[15264+m],sm_z_normed2[15263+m]])) # SM index is one day prior
    summary_frame = predictions.summary_frame(alpha=0.1)  # 90% confidence interval
    
    preds_mlr.append(np.float32(summary_frame['mean']))
    
    # Extract the confidence intervals
    preds_mlr_lower.append(np.float32(summary_frame['mean_ci_lower']))
    preds_mlr_upper.append(np.float32(summary_frame['mean_ci_upper']))
    
    # MLR models - Z500 and SM plus Z500*SM interaction term
    
    # same-day SM for the interaction term
    s2 = sm_z_normed2[mats_idx] 
    
    predict_df = pd.DataFrame({'z500':z,
                               'SM':s,
                               'interaction':z*s2})
    X = predict_df
    y = pd.DataFrame({'tmax':t})

    model = sm.OLS(y, X).fit() # fitting MLR model using statsmodels package
    
    interaction_term = zv[15264+m] * sm_z_normed2[15264+m] # SM index is same-day for interaction term
    
    # Get the predictions and confidence intervals
    predictions = model.get_prediction(np.array([zv[15264+m],sm_z_normed2[15263+m],interaction_term])) # SM index is one day prior
    summary_frame = predictions.summary_frame(alpha=0.1)  # 90% confidence interval
    
    preds_mlr2.append(np.float32(summary_frame['mean']))
    
    # Extract the confidence intervals
    preds_mlr_lower2.append(np.float32(summary_frame['mean_ci_lower']))
    preds_mlr_upper2.append(np.float32(summary_frame['mean_ci_upper']))
    
preds = np.squeeze(np.array(preds)) # predictions of detrended Tmax z-scores
preds_lower = np.squeeze(np.array(preds_lower)) # predictions of detrended Tmax z-scores
preds_upper = np.squeeze(np.array(preds_upper)) # predictions of detrended Tmax z-scores

# first, compute the detrended Tmax values that these z-scores represent
daily_std_detrended = np.load('daily_std_detrended.npy')
std_deg_C = daily_std_detrended[15264:15278]
dep_deg_C = std_deg_C * preds
daily_avg_detrended = np.load('daily_avg_detrended.npy')
pred_deg_C = daily_avg_detrended[15264:15278] + dep_deg_C

dep_deg_C_lower = std_deg_C * preds_lower
dep_deg_C_upper = std_deg_C * preds_upper
pred_deg_C_lower = daily_avg_detrended[15264:15278] + dep_deg_C_lower
pred_deg_C_upper = daily_avg_detrended[15264:15278] + dep_deg_C_upper

tmax_normed = np.load('tmax_normed.npy') 
tmax_normed_domain = np.nanmean(tmax_normed,axis=1) # observed domain-average Tmax in actual units (Deg C), latitude normalized
observed_tmax = tmax_normed_domain[15264:15278]

# convert predicted Tmax to anomalies
averages = daily_avg_detrended[15264:15278]
full_deps = observed_tmax - averages
anoms_circulation = pred_deg_C-averages

anoms_circulation_lower = pred_deg_C_lower-averages
anoms_circulation_upper = pred_deg_C_upper-averages

# repeat the above workflow for MLR

preds_mlr = np.squeeze(np.array(preds_mlr)) # predictions of detrended Tmax z-scores
preds_mlr_lower = np.squeeze(np.array(preds_mlr_lower)) # predictions of detrended Tmax z-scores
preds_mlr_upper = np.squeeze(np.array(preds_mlr_upper)) # predictions of detrended Tmax z-scores

# first, compute the detrended Tmax values that these z-scores represent
daily_std_detrended = np.load('daily_std_detrended.npy')
std_deg_C = daily_std_detrended[15264:15278]
dep_deg_C_mlr = std_deg_C * preds_mlr
daily_avg_detrended = np.load('daily_avg_detrended.npy')
pred_deg_C_mlr = daily_avg_detrended[15264:15278] + dep_deg_C_mlr

dep_deg_C_lower_mlr = std_deg_C * preds_mlr_lower
dep_deg_C_upper_mlr = std_deg_C * preds_mlr_upper
pred_deg_C_lower_mlr = daily_avg_detrended[15264:15278] + dep_deg_C_lower_mlr
pred_deg_C_upper_mlr = daily_avg_detrended[15264:15278] + dep_deg_C_upper_mlr

# add Tmax trend back in
trend_increments = np.load('tmax_trend_increments.npy')
this_increment = trend_increments[-1]
preds_trend_added_mlr = pred_deg_C_mlr + this_increment

preds_trend_added_mlr_lower = pred_deg_C_lower_mlr + this_increment
preds_trend_added_mlr_upper = pred_deg_C_upper_mlr + this_increment

# convert predicted Tmax to anomalies
anoms_mlr = pred_deg_C_mlr-averages
anoms_trend_added_mlr = preds_trend_added_mlr-averages

anoms_mlr_lower = pred_deg_C_lower_mlr-averages
anoms_mlr_upper = pred_deg_C_upper_mlr-averages
anoms_trend_added_mlr_lower = preds_trend_added_mlr_lower-averages
anoms_trend_added_mlr_upper = preds_trend_added_mlr_upper-averages

# repeat the above workflow for MLR with interaction term

preds_mlr2 = np.squeeze(np.array(preds_mlr2)) # predictions of detrended Tmax z-scores
preds_mlr_lower2 = np.squeeze(np.array(preds_mlr_lower2)) # predictions of detrended Tmax z-scores
preds_mlr_upper2 = np.squeeze(np.array(preds_mlr_upper2)) # predictions of detrended Tmax z-scores

# first, compute the detrended Tmax values that these z-scores represent
daily_std_detrended = np.load('daily_std_detrended.npy')
std_deg_C = daily_std_detrended[15264:15278]
dep_deg_C_mlr2 = std_deg_C * preds_mlr2
daily_avg_detrended = np.load('daily_avg_detrended.npy')
pred_deg_C_mlr2 = daily_avg_detrended[15264:15278] + dep_deg_C_mlr2

dep_deg_C_lower_mlr2 = std_deg_C * preds_mlr_lower2
dep_deg_C_upper_mlr2 = std_deg_C * preds_mlr_upper2
pred_deg_C_lower_mlr2 = daily_avg_detrended[15264:15278] + dep_deg_C_lower_mlr2
pred_deg_C_upper_mlr2 = daily_avg_detrended[15264:15278] + dep_deg_C_upper_mlr2

# add Tmax trend back in
trend_increments = np.load('tmax_trend_increments.npy')
this_increment = trend_increments[-1]
preds_trend_added_mlr2 = pred_deg_C_mlr2 + this_increment

preds_trend_added_mlr_lower2 = pred_deg_C_lower_mlr2 + this_increment
preds_trend_added_mlr_upper2 = pred_deg_C_upper_mlr2 + this_increment

# convert predicted Tmax to anomalies
anoms_mlr2 = pred_deg_C_mlr2-averages
anoms_trend_added_mlr2 = preds_trend_added_mlr2-averages

anoms_mlr_lower2 = pred_deg_C_lower_mlr2-averages
anoms_mlr_upper2 = pred_deg_C_upper_mlr2-averages
anoms_trend_added_mlr_lower2 = preds_trend_added_mlr_lower2-averages
anoms_trend_added_mlr_upper2 = preds_trend_added_mlr_upper2-averages

# plot Fig S7 - actual Tmax predictions

fig, ax1 = plt.subplots(figsize=(8,6))

ax1.set_ylabel('Domain-averaged T$_{\mathrm{max}}$ ($^\circ$C)', fontsize=16)
ax1.set_yticks(np.arange(30, 40.01, 1))
ax1.set_ylim(top=40)
ax1.set_ylim(bottom=30)
ax1.set_xticks([0, 3, 6, 9, 12])
ax1.set_xticklabels(['June 15', 'June 18', 'June 21', 'June 24', 'June 27'], fontsize=13)

line1, = ax1.plot(observed_tmax, '-o', color='firebrick', lw=2, marker='o', label='2023 T$_{\mathrm{max}}$')
line2, = ax1.plot(preds_trend_added_mlr2, '-o', color='tab:purple', lw=2, marker='s', label='pred(Z$_{\mathrm{500max}}$, SM$_{\mathrm{lag1}}$, Z$_{\mathrm{500max}}$*SM) + T$_{\mathrm{max}}$ trend')
line3, = ax1.plot(pred_deg_C_mlr2, '-o', color='cornflowerblue', lw=2, marker='^', label='pred(Z$_{\mathrm{500max}}$, SM$_{\mathrm{lag1}}$, Z$_{\mathrm{500max}}$*SM)')
line4, = ax1.plot(pred_deg_C_mlr, '-o', color='k', lw=2, marker='x', label='pred(Z$_{\mathrm{500max}}$, SM$_{\mathrm{lag1}}$)')
line5, = ax1.plot(pred_deg_C, '-o', color='0.5', lw=2, marker='d', label='pred(Z$_{\mathrm{500max}}$ only)')

ax1.fill_between(np.arange(14), pred_deg_C_lower, pred_deg_C_upper, color='0.5', alpha=0.4)
ax1.fill_between(np.arange(14), pred_deg_C_lower_mlr, pred_deg_C_upper_mlr, color='k', alpha=0.4)
ax1.fill_between(np.arange(14), pred_deg_C_lower_mlr2, pred_deg_C_upper_mlr2, color='cornflowerblue', alpha=0.4)
ax1.fill_between(np.arange(14), preds_trend_added_mlr_lower2, preds_trend_added_mlr_upper2, color='tab:purple', alpha=0.4)

ax1.tick_params(axis='y', labelsize=13)

# Add legend with custom order
l = ax1.legend(
    handles=[line1, line2, line3, line4, line5],  
    loc='upper left', 
    fontsize=11,  
    frameon=False  # No box around the legend
)

# Change legend text colors
colors = ['firebrick', 'tab:purple', 'cornflowerblue', 'k', '0.5']
for n, text in enumerate(l.get_texts()):
    text.set_color(colors[n])

# # Save the figure
# plt.savefig('fig_3_SI.pdf', dpi=600)

df_3b = pd.DataFrame({'observed':observed_tmax,
                      'z500_pred':pred_deg_C,
                      'z500_lower':pred_deg_C_lower,
                      'z500_upper':pred_deg_C_upper,
                      'mlr_pred':pred_deg_C_mlr,
                      'mlr_lower':pred_deg_C_lower_mlr,
                      'mlr_upper':pred_deg_C_upper_mlr,
                      'mlr_pred2':pred_deg_C_mlr2,
                      'mlr_lower2':pred_deg_C_lower_mlr2,
                      'mlr_upper2':pred_deg_C_upper_mlr2,
                      'trend_added2':preds_trend_added_mlr2,
                      'trend_added_lower2':preds_trend_added_mlr_lower2,
                      'trend_added_upper2':preds_trend_added_mlr_upper2})
dates = pd.date_range(start='6/15/2023', end='6/28/2023')
df_3b.set_index(dates,inplace=True)
df_3b = np.round(df_3b,2)

# plot fig 3 - Tmax anomalies

fig, ax1 = plt.subplots(figsize=(8, 6))

ax1.set_ylabel('Domain-averaged T$_{\mathrm{max}}$ anomalies ($^\circ$C)', fontsize=16)
ax1.set_yticks(np.arange(-1, 8.01, 1))
ax1.set_ylim(top=8)
ax1.set_ylim(bottom=-1)
ax1.set_xticks([0, 3, 6, 9, 12])
ax1.set_xticklabels(['June 15', 'June 18', 'June 21', 'June 24', 'June 27'], fontsize=13)

line1, = ax1.plot(full_deps, '-o', color='firebrick', lw=2, marker='o', label='2023 T$_{\mathrm{max}}$ anomaly')
line2, = ax1.plot(anoms_trend_added_mlr2, '-o', color='tab:purple', lw=2, marker='s', label='pred(Z$_{\mathrm{500max}}$, SM$_{\mathrm{lag1}}$, Z$_{\mathrm{500max}}$*SM) + T$_{\mathrm{max}}$ trend')
line3, = ax1.plot(anoms_mlr2, '-o', color='cornflowerblue', lw=2, marker='^', label='pred(Z$_{\mathrm{500max}}$, SM$_{\mathrm{lag1}}$, Z$_{\mathrm{500max}}$*SM)')
line4, = ax1.plot(anoms_mlr, '-o', color='k', lw=2, marker='x', label='pred(Z$_{\mathrm{500max}}$, SM$_{\mathrm{lag1}}$)')
line5, = ax1.plot(anoms_circulation, '-o', color='0.5', lw=2, marker='d', label='pred(Z$_{\mathrm{500max}}$ only)')

ax1.fill_between(np.arange(14), anoms_circulation_lower, anoms_circulation_upper, color='0.5', alpha=0.4)
ax1.fill_between(np.arange(14), anoms_mlr_lower, anoms_mlr_upper, color='k', alpha=0.4)
ax1.fill_between(np.arange(14), anoms_mlr_lower2, anoms_mlr_upper2, color='cornflowerblue', alpha=0.4)
ax1.fill_between(np.arange(14), anoms_trend_added_mlr_lower2, anoms_trend_added_mlr_upper2, color='tab:purple', alpha=0.4)

ax1.tick_params(axis='y', labelsize=13)

# Add legend with custom order
l = ax1.legend(
    handles=[line1, line2, line3, line4, line5],  
    loc='upper left', 
    fontsize=11,  
    frameon=False  # No box around the legend
)

# Change legend text colors
colors = ['firebrick', 'tab:purple', 'cornflowerblue', 'k', '0.5']
for n, text in enumerate(l.get_texts()):
    text.set_color(colors[n])

# Save the figure
plt.savefig('fig_3.pdf', dpi=600)

df_3c = pd.DataFrame({'observed':full_deps,
                      'z500_pred':anoms_circulation,
                      'z500_lower':anoms_circulation_lower,
                      'z500_upper':anoms_circulation_upper,
                      'mlr_pred':anoms_mlr,
                      'mlr_lower':anoms_mlr_lower,
                      'mlr_upper':anoms_mlr_upper,
                      'mlr_pred2':anoms_mlr2,
                      'mlr_lower2':anoms_mlr_lower2,
                      'mlr_upper2':anoms_mlr_upper2,
                      'trend_added2':anoms_trend_added_mlr2,
                      'trend_added_lower2':anoms_trend_added_mlr_lower2,
                      'trend_added_upper2':anoms_trend_added_mlr_upper2})
dates = pd.date_range(start='6/15/2023', end='6/28/2023')
df_3c.set_index(dates,inplace=True)
df_3c = np.round(df_3c,2)

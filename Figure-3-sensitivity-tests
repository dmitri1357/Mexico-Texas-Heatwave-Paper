#!/usr/bin/env python3

### sensitivity test for Figure 3 - how do results changed at diff. number of analogues from 20-100 

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt   
from scipy import stats
from sklearn.linear_model import LinearRegression
import statsmodels.api as sm
from tqdm import tqdm

# set font to Arial
import matplotlib as mpl
mpl.rc('font',family='Arial')

all_corrs = np.load('all_corrs.npy') # load daily pattern correlations file

# the following long loop is messy and involves lots of duplication,
# but I didn't want to mess with the code when testing sensitivity 
# so, I kept it exactly the same and simply reran 81 times (for 20-100 analogues)

tmax_detrended_z = np.load('tmax_detrended_z.npy')
tmax_z_normed2 = np.load('tmax_z_normed2.npy')
sm_z_normed2 = np.load('sm_z_normed2.npy')

num_analogues = np.arange(20,101,1)

# this loop takes 3 minutes to run
june20_z500_preds = []
june20_z500_sm_preds = []
june20_z500_sm_interaction_preds = []
for num in tqdm(num_analogues):
    
    dates = pd.date_range(start='1/1/1940', end='12/31/2023')
    dates = dates[~((dates.month == 2) & (dates.day == 29))]
    dates = pd.DataFrame(dates)
    dates['month'] = pd.to_datetime(dates.iloc[:,0]).dt.month
    dates = dates[dates.month.isin([4,5,6,7,8,9])]
    dates.reset_index(inplace=True)

    day_idx = np.arange(1,184,1)
    day_idxs = np.tile(day_idx,83)
    
    # match patterns for each day during 14-day heatwave period (June 15-28, 2023)
    for m in range(14):
        corrs = all_corrs[15264+m,:] # starting at day 15264 = 6/15/2023
    
        df1 = pd.DataFrame({'date':dates.iloc[:,1],'month':dates.month,
                            'corr':corrs})  
    
        df1['year'] = pd.to_datetime(df1.date).dt.year
        df2 = df1[df1.year<=2022]
        df2['day_idx'] = day_idxs
        df3 = df2[df2.day_idx.between(46+m,106+m)]
    
        dfx = df3.sort_values(by='corr',ascending=False)
        day_accum = []
        for k in range(df3.shape[0]):
            day = dfx.index.values[k]
            window = list(np.arange(day-3,day+4,1)) # 7-day window
            i, j = window[0], window[-1]
            res = any(ele >= i and ele <= j for ele in day_accum)
            if res == 0:
                day_accum.append(day)
        day_accum = np.array(day_accum)
    
        df5 = df3.loc[day_accum]
        df8 = df5.iloc[:num] # number of analogues is changed here
    
        mats_idx = df8.index.values
    
        np.save(f'mats_idx{m}',mats_idx) # save the index of matched days for each of the 14 days during heatwave
    
    # load domain-averaged variables
    z500_amjjas2_z = np.load('zvec_smaller.npy')
    z500_amjjas2_z = np.nanmax(z500_amjjas2_z,axis=1) # using max of detrended Z500 z-scores in smaller rectangle as predictor
    
    dates = pd.date_range(start='1/1/1940', end='12/31/2023')
    dates = dates[~((dates.month == 2) & (dates.day == 29))]
    dates = pd.DataFrame(dates)
    dates['month'] = pd.to_datetime(dates.iloc[:,0]).dt.month
    dates = dates[dates.month.isin([4,5,6,7,8,9])]
    idx = dates.index.values
    
    # run predictive models (simple and MLR) for analogs
    # these are done with SM lag -1 following revisions
    
    actuals = []
    preds = []
    preds_lower = []
    preds_upper = []
    preds_mlr = []
    preds_mlr_lower = []
    preds_mlr_upper = []
    preds_mlr2 = []
    preds_mlr_lower2 = []
    preds_mlr_upper2 = []
    for m in range(14):
        mats_idx = np.load(f'mats_idx{m}.npy') # for each of the 14 heatwave days, load the index of top-40 matched patterns (these were generated by previous code block above)
        
        zv = z500_amjjas2_z
        z = zv[mats_idx]
        tv = tmax_detrended_z # detrended tmax z-scores to train the models
        t = tv[mats_idx]
        
        day = zv[15264+m]
        day2 = tmax_z_normed2[15264+m] # actual z-scores (not detrended)
        actuals.append(day2)
        
        new_val = np.array(day)
        new_val = new_val.reshape(-1,1)
        
        X = pd.DataFrame({'z500':z})
        y = pd.DataFrame({'tmax':t})
            
        model = sm.OLS(y, X).fit()
             
        # Get the predictions and confidence intervals
        predictions = model.get_prediction(new_val)
        summary_frame = predictions.summary_frame(alpha=0.1)  # 90% confidence interval
        
        preds.append(np.float32(summary_frame['mean']))
        
        # Extract the confidence intervals
        preds_lower.append(np.float32(summary_frame['mean_ci_lower']))
        preds_upper.append(np.float32(summary_frame['mean_ci_upper']))
        
        # MLR models - Z500 and SM
        
        # changed SM index to -1 to indicate prior-day SM
        s = sm_z_normed2[mats_idx-1] # bringing in SM as 2nd predictor
        
        predict_df = pd.DataFrame({'z500':z,
                                   'SM':s})
        X = predict_df
        y = pd.DataFrame({'tmax':t})
    
        model = sm.OLS(y, X).fit() # fitting MLR model using statsmodels package
        
        # Get the predictions and confidence intervals
        predictions = model.get_prediction(np.array([zv[15264+m],sm_z_normed2[15263+m]])) # SM index is one day prior
        summary_frame = predictions.summary_frame(alpha=0.1)  # 90% confidence interval
        
        preds_mlr.append(np.float32(summary_frame['mean']))
        
        # Extract the confidence intervals
        preds_mlr_lower.append(np.float32(summary_frame['mean_ci_lower']))
        preds_mlr_upper.append(np.float32(summary_frame['mean_ci_upper']))
        
        # MLR models - Z500 and SM plus Z500*SM interaction term
        
        # same-day SM for the interaction term
        s2 = sm_z_normed2[mats_idx] 
        
        predict_df = pd.DataFrame({'z500':z,
                                   'SM':s,
                                   'interaction':z*s2})
        X = predict_df
        y = pd.DataFrame({'tmax':t})
    
        model = sm.OLS(y, X).fit() # fitting MLR model using statsmodels package
        
        interaction_term = zv[15264+m] * sm_z_normed2[15264+m] # SM index is same-day for interaction term
        
        # Get the predictions and confidence intervals
        predictions = model.get_prediction(np.array([zv[15264+m],sm_z_normed2[15263+m],interaction_term])) # SM index is one day prior
        summary_frame = predictions.summary_frame(alpha=0.1)  # 90% confidence interval
        
        preds_mlr2.append(np.float32(summary_frame['mean']))
        
        # Extract the confidence intervals
        preds_mlr_lower2.append(np.float32(summary_frame['mean_ci_lower']))
        preds_mlr_upper2.append(np.float32(summary_frame['mean_ci_upper']))
        
    preds = np.squeeze(np.array(preds)) # predictions of detrended Tmax z-scores
    preds_lower = np.squeeze(np.array(preds_lower)) # predictions of detrended Tmax z-scores
    preds_upper = np.squeeze(np.array(preds_upper)) # predictions of detrended Tmax z-scores
    
    # first, compute the detrended Tmax values that these z-scores represent
    daily_std_detrended = np.load('daily_std_detrended.npy')
    std_deg_C = daily_std_detrended[15264:15278]
    dep_deg_C = std_deg_C * preds
    daily_avg_detrended = np.load('daily_avg_detrended.npy')
    pred_deg_C = daily_avg_detrended[15264:15278] + dep_deg_C
    
    dep_deg_C_lower = std_deg_C * preds_lower
    dep_deg_C_upper = std_deg_C * preds_upper
    pred_deg_C_lower = daily_avg_detrended[15264:15278] + dep_deg_C_lower
    pred_deg_C_upper = daily_avg_detrended[15264:15278] + dep_deg_C_upper
    
    tmax_normed = np.load('tmax_normed.npy') 
    tmax_normed_domain = np.nanmean(tmax_normed,axis=1) # observed domain-average Tmax in actual units (Deg C), latitude normalized
    observed_tmax = tmax_normed_domain[15264:15278]
    
    # convert predicted Tmax to anomalies
    averages = daily_avg_detrended[15264:15278]
    full_deps = observed_tmax - averages
    anoms_circulation = pred_deg_C-averages
    
    anoms_circulation_lower = pred_deg_C_lower-averages
    anoms_circulation_upper = pred_deg_C_upper-averages
    
    # repeat the above workflow for MLR
    
    preds_mlr = np.squeeze(np.array(preds_mlr)) # predictions of detrended Tmax z-scores
    preds_mlr_lower = np.squeeze(np.array(preds_mlr_lower)) # predictions of detrended Tmax z-scores
    preds_mlr_upper = np.squeeze(np.array(preds_mlr_upper)) # predictions of detrended Tmax z-scores
    
    # first, compute the detrended Tmax values that these z-scores represent
    daily_std_detrended = np.load('daily_std_detrended.npy')
    std_deg_C = daily_std_detrended[15264:15278]
    dep_deg_C_mlr = std_deg_C * preds_mlr
    daily_avg_detrended = np.load('daily_avg_detrended.npy')
    pred_deg_C_mlr = daily_avg_detrended[15264:15278] + dep_deg_C_mlr
    
    dep_deg_C_lower_mlr = std_deg_C * preds_mlr_lower
    dep_deg_C_upper_mlr = std_deg_C * preds_mlr_upper
    pred_deg_C_lower_mlr = daily_avg_detrended[15264:15278] + dep_deg_C_lower_mlr
    pred_deg_C_upper_mlr = daily_avg_detrended[15264:15278] + dep_deg_C_upper_mlr
    
    # add Tmax trend back in
    trend_increments = np.load('tmax_trend_increments.npy')
    this_increment = trend_increments[-1]
    preds_trend_added_mlr = pred_deg_C_mlr + this_increment
    
    preds_trend_added_mlr_lower = pred_deg_C_lower_mlr + this_increment
    preds_trend_added_mlr_upper = pred_deg_C_upper_mlr + this_increment
    
    # convert predicted Tmax to anomalies
    anoms_mlr = pred_deg_C_mlr-averages
    anoms_trend_added_mlr = preds_trend_added_mlr-averages
    
    anoms_mlr_lower = pred_deg_C_lower_mlr-averages
    anoms_mlr_upper = pred_deg_C_upper_mlr-averages
    anoms_trend_added_mlr_lower = preds_trend_added_mlr_lower-averages
    anoms_trend_added_mlr_upper = preds_trend_added_mlr_upper-averages
    
    # repeat the above workflow for MLR with interaction term
    
    preds_mlr2 = np.squeeze(np.array(preds_mlr2)) # predictions of detrended Tmax z-scores
    preds_mlr_lower2 = np.squeeze(np.array(preds_mlr_lower2)) # predictions of detrended Tmax z-scores
    preds_mlr_upper2 = np.squeeze(np.array(preds_mlr_upper2)) # predictions of detrended Tmax z-scores
    
    # first, compute the detrended Tmax values that these z-scores represent
    daily_std_detrended = np.load('daily_std_detrended.npy')
    std_deg_C = daily_std_detrended[15264:15278]
    dep_deg_C_mlr2 = std_deg_C * preds_mlr2
    daily_avg_detrended = np.load('daily_avg_detrended.npy')
    pred_deg_C_mlr2 = daily_avg_detrended[15264:15278] + dep_deg_C_mlr2
    
    dep_deg_C_lower_mlr2 = std_deg_C * preds_mlr_lower2
    dep_deg_C_upper_mlr2 = std_deg_C * preds_mlr_upper2
    pred_deg_C_lower_mlr2 = daily_avg_detrended[15264:15278] + dep_deg_C_lower_mlr2
    pred_deg_C_upper_mlr2 = daily_avg_detrended[15264:15278] + dep_deg_C_upper_mlr2
    
    # add Tmax trend back in
    trend_increments = np.load('tmax_trend_increments.npy')
    this_increment = trend_increments[-1]
    preds_trend_added_mlr2 = pred_deg_C_mlr2 + this_increment
    
    preds_trend_added_mlr_lower2 = pred_deg_C_lower_mlr2 + this_increment
    preds_trend_added_mlr_upper2 = pred_deg_C_upper_mlr2 + this_increment
    
    # convert predicted Tmax to anomalies
    anoms_mlr2 = pred_deg_C_mlr2-averages
    anoms_trend_added_mlr2 = preds_trend_added_mlr2-averages
    
    anoms_mlr_lower2 = pred_deg_C_lower_mlr2-averages
    anoms_mlr_upper2 = pred_deg_C_upper_mlr2-averages
    anoms_trend_added_mlr_lower2 = preds_trend_added_mlr_lower2-averages
    anoms_trend_added_mlr_upper2 = preds_trend_added_mlr_upper2-averages
    
    df_3c = pd.DataFrame({'observed':full_deps,
                          'z500_pred':anoms_circulation,
                          'z500_lower':anoms_circulation_lower,
                          'z500_upper':anoms_circulation_upper,
                          'mlr_pred':anoms_mlr,
                          'mlr_lower':anoms_mlr_lower,
                          'mlr_upper':anoms_mlr_upper,
                          'mlr_pred2':anoms_mlr2,
                          'mlr_lower2':anoms_mlr_lower2,
                          'mlr_upper2':anoms_mlr_upper2,
                          'trend_added2':anoms_trend_added_mlr2,
                          'trend_added_lower2':anoms_trend_added_mlr_lower2,
                          'trend_added_upper2':anoms_trend_added_mlr_upper2})
    dates = pd.date_range(start='6/15/2023', end='6/28/2023')
    df_3c.set_index(dates,inplace=True)
    df_3c = np.round(df_3c,2)

    june20_z500_preds.append(df_3c.iloc[5,1]) # row 5 is 6/20/2023, column 1 is z500 pred
    june20_z500_sm_preds.append(df_3c.iloc[5,4]) # row 5 is 6/20/2023, column 4 is z500/SM pred
    june20_z500_sm_interaction_preds.append(df_3c.iloc[5,7]) # row 5 is 6/20/2023, column 7 is z500/SM/interaction pred

# plot the results - this is Fig. S6

np.percentile(june20_z500_preds,25)
np.percentile(june20_z500_preds,75)

plt.figure(figsize=(8, 5))
plt.plot(num_analogues, june20_z500_preds, linestyle='-', lw=3, color='0.5')
plt.plot(num_analogues, june20_z500_sm_preds, linestyle='-', lw=3, color='k')
plt.plot(num_analogues, june20_z500_sm_interaction_preds, linestyle='-', lw=3, color='cornflowerblue')
plt.title('Sensitivity of predicted T$_{\mathrm{max}}$ anomaly \n on 20 June 2023 to number of analogues', fontsize=18)
plt.xlabel('Number of analogues', fontsize=12)
plt.ylabel('Domain-averaged T$_{\mathrm{max}}$ anomaly ($^\circ$C)', fontsize=12)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
#plt.legend(fontsize=10)
#plt.grid(alpha=0.3)
plt.show()

np.percentile(june20_z500_preds,25)
np.percentile(june20_z500_preds,75)
np.percentile(june20_z500_sm_preds,25)
np.percentile(june20_z500_sm_preds,75)
np.percentile(june20_z500_sm_interaction_preds,25)
np.percentile(june20_z500_sm_interaction_preds,75)

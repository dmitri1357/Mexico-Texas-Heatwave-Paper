#!/usr/bin/env python3

import numpy as np
import pandas as pd
import xarray as xr
from tqdm import tqdm
import matplotlib.pyplot as plt   
from mpl_toolkits.basemap import Basemap

import matplotlib as mpl
mpl.rc('font',family='Arial')

# detrending tmax the new way (during peer review) by removing the global-average timeseries from Berkeley Earth

tmax = np.load('tmax_1940_2022.npy')
tmax_2023 = np.load('tmax_2023.npy')

# naming it z500 to be consistent with the rest of the code below
# which was copied over from the Z500 code
# not a best practice, but this was convenient and ensured the code would run exactly the same way
z500 = np.concatenate([tmax,tmax_2023],axis=0)

dates = pd.date_range(start='1/1/1940', end='12/31/2023')
dates = dates[~((dates.month == 2) & (dates.day == 29))]
dates = pd.DataFrame(dates)
dates['month'] = pd.to_datetime(dates.iloc[:,0]).dt.month
dates = dates[dates.month.isin([4,5,6,7,8,9])]
idx = dates.index.values

z500 = z500[idx,:,:]

var = z500[15269,:] # day 15269 is 6/20/2023
var = np.flipud(var)
lat = np.arange(5,45.01,0.25)
lon = np.arange(-125,-74.99,0.25)
lon, lat = np.meshgrid(lon, lat)

plt.figure(figsize=(10, 8))
m = Basemap(projection='lcc', area_thresh=10000, resolution='l',
            width=5E6, height=5E6,
            lat_0=25, lon_0=-100)
m.fillcontinents(color='white',lake_color='white',alpha=1,zorder=0)
m.drawstates(linewidth=0.5,linestyle='solid',color='k')
m.drawcountries(linewidth=0.5,linestyle='solid',color='k')
m.drawcoastlines(color='black')
m.pcolormesh(lon, lat, var, latlon=True, cmap=plt.cm.get_cmap('rainbow'))
plt.colorbar()

lat = np.flipud(np.arange(5,45.01,0.25))

z500 = z500[:,33:122,26:154] # lat subset coords for upside down array

# flip z500 to match area_vec
z_flipped = np.empty([15372,89,128])
for k in range(15372):
    day = z500[k,:,:]
    z_flipped[k,:,:] = np.flipud(day)

zvec = np.reshape(z_flipped,(15372,11392))

# latitude-normalize 
area_vec = np.load('area_vec.npy') # this contains areas of each grid cell in sq km for area-weighting

z500_normed = np.empty([15372,11392])
for k in range(15372):
    day = zvec[k,:]
    z500_normed[k,:] = day/area_vec

zmeans = np.nanmean(z500_normed,axis=1)

split = np.split(zmeans,84)
ann_averages = [np.nanmean(split[k]) for k in range(84)]

plt.plot(ann_averages)

berk_temps = np.load('berk_temps.npy') # load Berkeley Earth global temp. time series for detrending

trendline = []
for i in range(84):
    if i == 0:
        trendline.append(0)
    else:
        trendline.append(berk_temps[i]-berk_temps[0])
plt.plot(trendline)

np.save('tmax_trend_increments',trendline)

# detrending tmax
tmax = np.load('tmax_1940_2022.npy')
tmax_2023 = np.load('tmax_2023.npy')

# naming it z500_all to be consistent with the rest of the code below
# which was copied from the Z500 code
z500_all = np.concatenate([tmax,tmax_2023],axis=0)

z_reshape = np.reshape(z500_all,(84,365,161,201))
z_reshape = np.reshape(z_reshape,(84,365,32361))

# running this code for entire year even though I only care about Apr-Sept --> so I can calculate z-scores from moving windows
detrended_z500 = np.empty([84,365,32361])
for i in tqdm(range(84)):
    yr_vals = z_reshape[i,:,:]
    trend_val = trendline[i]
    detrended_z500[i,:,:] = yr_vals - trend_val

detrended_z500 = np.reshape(detrended_z500,(30660,32361))

np.save('detrended_tmax',detrended_z500.astype('float32'))

tmax = np.load('detrended_tmax.npy')

dates = pd.date_range(start='1/1/1940', end='12/31/2023')
dates = dates[~((dates.month == 2) & (dates.day == 29))]
dates = pd.DataFrame(dates)
dates['month'] = pd.to_datetime(dates.iloc[:,0]).dt.month
dates = dates[dates.month.isin([4,5,6,7,8,9])]
idx = dates.index.values

tmax = tmax[idx,:]
tmax = tmax.T

# flip the lats 
tvec = np.reshape(tmax,(161,201,15372))
tmax_flipped = np.empty([161,201,15372])
for k in tqdm(range(15372)):
    day = tvec[:,:,k]
    tmax_flipped[:,:,k] = np.flipud(day)

var = tmax_flipped[:,:,15269] # 6/20/2003
lat = np.arange(5,45.01,0.25)
lon = np.arange(-125,-74.99,0.25)
lon, lat = np.meshgrid(lon, lat)

plt.figure(figsize=(10, 8))
m = Basemap(projection='lcc', area_thresh=10000, resolution='l',
            width=6E6, height=6E6,
            lat_0=25, lon_0=-100)
m.fillcontinents(color='white',lake_color='white',alpha=1,zorder=0)
m.drawstates(linewidth=0.5,linestyle='solid',color='k')
m.drawcountries(linewidth=0.5,linestyle='solid',color='k')
m.drawcoastlines(color='black')
m.pcolormesh(lon, lat, var, latlon=True, cmap=plt.cm.get_cmap('rainbow'))
plt.colorbar()
plt.title('Detrended Tmax on June 20, 2023', size=18)

tmax_cropped = tmax_flipped[39:128,26:154,:]

var = tmax_cropped[:,:,15269] # 6/20/2003
lat = np.arange(14.75,36.76,0.25)
lon = np.arange(-118.5,-86.74,0.25)
lon, lat = np.meshgrid(lon, lat)

plt.figure(figsize=(10, 8))
m = Basemap(projection='lcc', area_thresh=10000, resolution='l',
            width=3.7E6, height=3.3E6,
            lat_0=25, lon_0=-102.6)
m.fillcontinents(color='white',lake_color='white',alpha=1,zorder=0)
m.drawstates(linewidth=0.5,linestyle='solid',color='k')
m.drawcountries(linewidth=0.5,linestyle='solid',color='k')
m.drawcoastlines(color='black')
m.pcolormesh(lon, lat, var, latlon=True, cmap=plt.cm.get_cmap('rainbow'))
plt.title('Detrended Tmax on June 20, 2023', size=18)
plt.colorbar()

area_vec = np.load('area_vec.npy')

# latitude-normalize 
tvec = np.reshape(tmax_cropped,(11392,15372))
tvec = tvec.T

tmax_z_normed = np.empty([15372,11392])
for k in range(15372):
    day = tvec[k,:]
    tmax_z_normed[k,:] = day/area_vec

# subset by domain

tmax_amjjas = tmax_z_normed
tmax_amjjas = tmax_amjjas.T

june20 = tmax_amjjas[:,15269]

var = np.reshape(june20,(89,128))
lat = np.arange(14.75,36.76,0.25)
lon = np.arange(-118.5,-86.74,0.25)
lon, lat = np.meshgrid(lon, lat)

plt.figure(figsize=(10, 8))
m = Basemap(projection='lcc', area_thresh=10000, resolution='l',
            width=5E6, height=5E6,
            lat_0=25, lon_0=-100)
m.fillcontinents(color='gray',lake_color='white',alpha=1,zorder=0)
m.drawstates(linewidth=0.5,linestyle='solid',color='k')
m.drawcountries(linewidth=0.5,linestyle='solid',color='k')
m.drawcoastlines(color='black')
m.pcolormesh(lon, lat, var, latlon=True, cmap=plt.cm.get_cmap('rainbow'))
plt.colorbar()

# convert one day to raster
# reusing june20
day = np.reshape(june20,(89,128))

# need to flip it back to being upside down for raster
day = np.flipud(day)

import rasterio
from rasterio.transform import from_origin

Z = day
new_dataset = rasterio.open(
        'june20_tmax_normed_det.tif',
        'w',
        driver='GTiff',
        height=Z.shape[0],
        width=Z.shape[1],
        count=1,
        dtype=Z.dtype,
        transform=from_origin(-118.5, 36.75, 0.25, 0.25),
        crs='+proj=latlong')

new_dataset.write(Z, 1)
new_dataset.close()

import fiona
import rasterio
import rasterio.mask

with fiona.open("texas_mexico.shp", "r") as shapefile:
    shapes = [feature["geometry"] for feature in shapefile]

with rasterio.open("june20_tmax_normed_det.tif") as src:
    out_image, out_transform = rasterio.mask.mask(src, shapes, crop=True, nodata=np.nan)
    out_meta = src.meta

out_image = np.squeeze(out_image)

plt.imshow(out_image)

# referencing the "out_transform" variable: 
# left bound is -118.5, top bound is 36.75

var = np.flipud(out_image)

lat = np.arange(14.75,36.76,0.25)
lon = np.arange(-118.5,-86.74,0.25)
lon, lat = np.meshgrid(lon, lat)

plt.figure(figsize=(10, 8))
m = Basemap(projection='lcc', area_thresh=10000, resolution='l',
            width=3.3E6, height=3.3E6,
            lat_0=25, lon_0=-102)
m.fillcontinents(color='white',lake_color='white',alpha=1,zorder=0)
m.drawstates(linewidth=0.5,linestyle='solid',color='k')
m.drawcountries(linewidth=0.5,linestyle='solid',color='k')
m.drawcoastlines(color='black')
m.pcolormesh(lon, lat, var, latlon=True, cmap=plt.cm.get_cmap('rainbow'))
plt.colorbar()

tmax_full_extent = var

# removing random grid cells north of Texas
tmax_full_extent[79,76] = np.nan
tmax_full_extent[77,83] = np.nan
tmax_full_extent[77,87] = np.nan
tmax_full_extent[77,91] = np.nan
tmax_full_extent[77,93] = np.nan

# removing random grid cells north of Baja
tmax_full_extent[72,10:15] = np.nan

# removing other random ones
tmax_full_extent[28,47] = np.nan
tmax_full_extent[40,25] = np.nan
tmax_full_extent[42,25] = np.nan
tmax_full_extent[52,31] = np.nan
tmax_full_extent[57,24] = np.nan
tmax_full_extent[58,24] = np.nan
tmax_full_extent[59,20] = np.nan

var2 = tmax_full_extent
lat = np.arange(14.75,36.76,0.25)
lon = np.arange(-118.5,-86.74,0.25)
lon, lat = np.meshgrid(lon, lat)

plt.figure(figsize=(10, 8))
m = Basemap(projection='lcc', area_thresh=10000, resolution='l',
            width=3.3E6, height=3.3E6,
            lat_0=25, lon_0=-102)
m.fillcontinents(color='white',lake_color='white',alpha=1,zorder=0)
m.drawstates(linewidth=0.5,linestyle='solid',color='k')
m.drawcountries(linewidth=0.5,linestyle='solid',color='k')
m.drawcoastlines(color='black')
m.pcolormesh(lon, lat, var2, latlon=True, cmap=plt.cm.get_cmap('rainbow'))
plt.colorbar()

# subset tmax_amjjas by raster

raster_vec = np.reshape(tmax_full_extent,(11392))
np.save('raster_vec_smaller',raster_vec)
tmax_amjjas2 = np.empty([11392,15372])
for k in tqdm(range(15372)):
    day = tmax_amjjas[:,k]
    tmax_amjjas2[:,k] = np.where(~np.isnan(raster_vec),day,raster_vec)
    
tmax_amjjas2 = np.swapaxes(tmax_amjjas2,1,0)

tvec = np.reshape(tmax_amjjas2,(15372,89,128))

tmax_subset = tvec

# map to make sure things look correct
var = tmax_subset[15269,:]

lat = np.arange(14.75,36.76,0.25)
lon = np.arange(-118.5,-86.74,0.25)
lon, lat = np.meshgrid(lon, lat)

plt.figure(figsize=(10, 8))
m = Basemap(projection='lcc', area_thresh=10000, resolution='l',
            width=3.3E6, height=3.3E6,
            lat_0=25, lon_0=-102)
m.fillcontinents(color='white',lake_color='white',alpha=1,zorder=0)
m.drawstates(linewidth=0.5,linestyle='solid',color='k')
m.drawcountries(linewidth=0.5,linestyle='solid',color='k')
m.drawcoastlines(color='black')
m.pcolormesh(lon, lat, var, latlon=True, cmap=plt.cm.get_cmap('rainbow'))
plt.colorbar()

tmax_subset = np.reshape(tmax_subset,(15372,11392))
tmax_normed_detrended = np.nanmean(tmax_subset,axis=1)

np.save('tmax_normed_detrended',tmax_normed_detrended.astype('float32'))

# calculating standard deviations of detrended Tmax (1991-2020)

tmax_normed_detrended = np.load('tmax_normed_detrended.npy')

# get 1991-2020 index
dates = pd.date_range(start='1/1/1940', end='12/31/2023')
dates = dates[~((dates.month == 2) & (dates.day == 29))]
dates = pd.DataFrame(dates)
dates['year'] = pd.to_datetime(dates.iloc[:,0]).dt.year
dates['month'] = pd.to_datetime(dates.iloc[:,0]).dt.month
dates = dates[dates.month.isin([4,5,6,7,8,9])]
dates.reset_index(inplace=True)

dates = dates[dates.year.isin(np.arange(1991,2021,1))]
idx = dates.index.values

tmax_1991_2020 = tmax_normed_detrended[idx] 

from mvgavg import mvgavg

# computing 15-day windows for 1991-2020
cell = list(tmax_1991_2020)
averages = list(mvgavg(cell,15))
x1 = cell[:7]
x2 = averages
x3 = cell[-7:]
x2.extend(x3)
x1.extend(x2)

tmax_15d = x1

# compute daily averages of 15-day moving average for 1991-2020
daily_avg = np.empty(183)
cell = np.array(tmax_15d)
years = np.split(cell,30)
for i in range(183):
    x1 = years[0][i]
    x2 = years[1][i]
    x3 = years[2][i]
    x4 = years[3][i]
    x5 = years[4][i]
    x6 = years[5][i]
    x7 = years[6][i]
    x8 = years[7][i]
    x9 = years[8][i]
    x10 = years[9][i]
    x11 = years[10][i]
    x12 = years[11][i]
    x13 = years[12][i]
    x14 = years[13][i]
    x15 = years[14][i]
    x16 = years[15][i]
    x17 = years[16][i]
    x18 = years[17][i]
    x19 = years[18][i]
    x20 = years[19][i]
    x21 = years[20][i]
    x22 = years[21][i]
    x23 = years[22][i]
    x24 = years[23][i]
    x25 = years[24][i]
    x26 = years[25][i]
    x27 = years[26][i]
    x28 = years[27][i]
    x29 = years[28][i]
    x30 = years[29][i]
    allvals = [x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16,x17,x18,x19,x20,
               x21,x22,x23,x24,x25,x26,x27,x28,x29,x30]
    daily_avg[i] = np.mean(allvals)

daily_std = np.empty(169) # minus 14 days (7 + 7)
cell = np.array(tmax_15d)
years = np.split(cell,30)
for i in tqdm(range(169)):
    x1 = years[0][i:i+15]
    x2 = years[1][i:i+15]
    x3 = years[2][i:i+15]
    x4 = years[3][i:i+15]
    x5 = years[4][i:i+15]
    x6 = years[5][i:i+15]
    x7 = years[6][i:i+15]
    x8 = years[7][i:i+15]
    x9 = years[8][i:i+15]
    x10 = years[9][i:i+15]
    x11 = years[10][i:i+15]
    x12 = years[11][i:i+15]
    x13 = years[12][i:i+15]
    x14 = years[13][i:i+15]
    x15 = years[14][i:i+15]
    x16 = years[15][i:i+15]
    x17 = years[16][i:i+15]
    x18 = years[17][i:i+15]
    x19 = years[18][i:i+15]
    x20 = years[19][i:i+15]
    x21 = years[20][i:i+15]
    x22 = years[21][i:i+15]
    x23 = years[22][i:i+15]
    x24 = years[23][i:i+15]
    x25 = years[24][i:i+15]
    x26 = years[25][i:i+15]
    x27 = years[26][i:i+15]
    x28 = years[27][i:i+15]
    x29 = years[28][i:i+15]
    x30 = years[29][i:i+15]
    allvals = [x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16,x17,x18,x19,x20,
               x21,x22,x23,x24,x25,x26,x27,x28,x29,x30]
    allvals2 = []
    for j in range(30):
        allvals2.extend(allvals[j])
    daily_std[i] = np.std(allvals2)

std_fill1 = np.zeros(7)
std_fill2 = np.zeros(7)
daily_std = np.hstack([std_fill1,daily_std,std_fill2])
daily_std[:7] = 1000 # changed from 1 to 1000
daily_std[-7:] = 1000 # changed from 1 to 1000

# compute for for 1940-2023

all_avg = np.tile(daily_avg,84)
all_std = np.tile(daily_std,84)
tmax_detrended_z = (tmax_normed_detrended - all_avg)/all_std

np.save('tmax_detrended_z',tmax_detrended_z.astype('float32'))
tmax_detrended_z = np.load('tmax_detrended_z.npy')

np.save('daily_avg_detrended',all_avg.astype('float32'))
np.save('daily_std_detrended',all_std.astype('float32'))

### calculate domain-averaged Tmax z-scores (actual, not detrended)

tmax_1940_2022 = np.load('tmax_1940_2022.npy')
tmax_2023 = np.load('tmax_2023.npy')
tmax_all = np.concatenate([tmax_1940_2022,tmax_2023],axis=0)

# sanity check
var = tmax_2023[170,:]
var = np.flipud(var)

lat = np.arange(5,45.01,0.25)
lon = np.arange(-125,-74.99,0.25)
lon, lat = np.meshgrid(lon, lat)

plt.figure(figsize=(10, 8))
m = Basemap(projection='lcc', area_thresh=10000, resolution='l',
            width=3.3E6, height=3.3E6,
            lat_0=25, lon_0=-102)
m.fillcontinents(color='white',lake_color='white',alpha=1,zorder=0)
m.drawstates(linewidth=0.5,linestyle='solid',color='k')
m.drawcountries(linewidth=0.5,linestyle='solid',color='k')
m.drawcoastlines(color='black')
m.pcolormesh(lon, lat, var, latlon=True, cmap=plt.cm.get_cmap('rainbow'))
plt.colorbar()
plt.clim(20,45)

tmax = tmax_all
tmax = np.reshape(tmax,(30660,32361))
tmax = np.swapaxes(tmax,1,0)

# flip lats 
tvec = np.reshape(tmax,(161,201,30660))
tmax_flipped = np.empty([161,201,30660])
for k in range(30660):
    day = tvec[:,:,k]
    tmax_flipped[:,:,k] = np.flipud(day)

raster_vec = np.load('raster_vec.npy') # this file contains nan values for all grid cells outside the Mexico-Texas domain, for subsetting

tmax_amjjas = np.reshape(tmax_flipped,(32361,30660))
tmax_amjjas2 = np.empty([32361,30660])
for k in tqdm(range(30660)):
    day = tmax_amjjas[:,k]
    tmax_amjjas2[:,k] = np.where(~np.isnan(raster_vec),day,raster_vec)
    
tmax_amjjas2 = np.swapaxes(tmax_amjjas2,1,0)

tvec = np.reshape(tmax_amjjas2,(30660,161,201))

# subset back to bbox to make smaller array
tmax_subset = tvec[:,39:128,26:154]

# map to make sure things look correct
var = tmax_subset[30465,:]

lat = np.arange(14.75,36.76,0.25)
lon = np.arange(-118.5,-86.74,0.25)
lon, lat = np.meshgrid(lon, lat)

plt.figure(figsize=(10, 8))
m = Basemap(projection='lcc', area_thresh=10000, resolution='l',
            width=3.3E6, height=3.3E6,
            lat_0=25, lon_0=-102)
m.fillcontinents(color='white',lake_color='white',alpha=1,zorder=0)
m.drawstates(linewidth=0.5,linestyle='solid',color='k')
m.drawcountries(linewidth=0.5,linestyle='solid',color='k')
m.drawcoastlines(color='black')
m.pcolormesh(lon, lat, var, latlon=True, cmap=plt.cm.get_cmap('rainbow'))
plt.colorbar()
plt.clim(20,45)
#plt.clim(-3,3)

tmax_amjjas2 = tmax_subset.astype('float32')

# latitude-normalize 

area_vec = np.load('area_vec.npy')

tvec = np.reshape(tmax_amjjas2,(30660,11392))

tmax_normed = np.empty([30660,11392])
for k in range(30660):
    day = tvec[k,:]
    tmax_normed[k,:] = day/area_vec

tmax_normed = tmax_normed.astype('float32')

np.save('tmax_normed2',tmax_normed)
tmax_normed2 = np.load('tmax_normed2.npy')

# get 1991-2020 index
dates = pd.date_range(start='1/1/1940', end='12/31/2023')
dates = dates[~((dates.month == 2) & (dates.day == 29))]
dates = pd.DataFrame(dates)
dates['year'] = pd.to_datetime(dates.iloc[:,0]).dt.year
dates = dates[dates.year.isin(np.arange(1990,2022,1))]
idx = dates.index.values

tmax_1990_2021 = tmax_normed2[idx,:] # grabbing 1990 and 2021 to complete 15-day moving averages

vec = np.nanmean(tmax_1990_2021,axis=1)

# computing 15-day windows for 1990-2021, then will subset to 1991-2020
from mvgavg import mvgavg
z500_15d = np.empty(11680)
cell = list(vec)
averages = list(mvgavg(cell,15))
x1 = cell[:7]
x2 = averages
x3 = cell[-7:]
x2.extend(x3)
x1.extend(x2)
z500_15d = x1

def moving_window(num_year,window_length):
    z500_15d = np.empty((num_year+2)*365)
    cell = list(vec)
    averages = list(mvgavg(cell,window_length))
    x1 = cell[:int((window_length-1)/2)]
    x2 = averages
    x3 = cell[int((window_length-1)/2):]
    x2.extend(x3)
    x1.extend(x2)
    return x1

z500_15d_2 = moving_window(30,15)

# drop 1990 and 2021
z500_15d = z500_15d[365:]
z500_15d = z500_15d[:-365]

# compute daily averages of 15-day moving average for 1991-2020
daily_avg = np.empty(365)
cell = np.array(z500_15d)
years = np.split(cell,30)
for i in range(365):
    x1 = years[0][i]
    x2 = years[1][i]
    x3 = years[2][i]
    x4 = years[3][i]
    x5 = years[4][i]
    x6 = years[5][i]
    x7 = years[6][i]
    x8 = years[7][i]
    x9 = years[8][i]
    x10 = years[9][i]
    x11 = years[10][i]
    x12 = years[11][i]
    x13 = years[12][i]
    x14 = years[13][i]
    x15 = years[14][i]
    x16 = years[15][i]
    x17 = years[16][i]
    x18 = years[17][i]
    x19 = years[18][i]
    x20 = years[19][i]
    x21 = years[20][i]
    x22 = years[21][i]
    x23 = years[22][i]
    x24 = years[23][i]
    x25 = years[24][i]
    x26 = years[25][i]
    x27 = years[26][i]
    x28 = years[27][i]
    x29 = years[28][i]
    x30 = years[29][i]
    allvals = [x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16,x17,x18,x19,x20,
               x21,x22,x23,x24,x25,x26,x27,x28,x29,x30]
    daily_avg[i] = np.mean(allvals)

np.save('daily_avg_tmax2',daily_avg)

# subset "vec" to 1991-2020 so that sd's are only from this period
vec = vec[365:]
vec = vec[:-365]

daily_std = np.empty(351) # minus 14 days (7 + 7)
cell = vec
years = np.split(cell,30)
for i in range(351):
    x1 = years[0][i:i+15]
    x2 = years[1][i:i+15]
    x3 = years[2][i:i+15]
    x4 = years[3][i:i+15]
    x5 = years[4][i:i+15]
    x6 = years[5][i:i+15]
    x7 = years[6][i:i+15]
    x8 = years[7][i:i+15]
    x9 = years[8][i:i+15]
    x10 = years[9][i:i+15]
    x11 = years[10][i:i+15]
    x12 = years[11][i:i+15]
    x13 = years[12][i:i+15]
    x14 = years[13][i:i+15]
    x15 = years[14][i:i+15]
    x16 = years[15][i:i+15]
    x17 = years[16][i:i+15]
    x18 = years[17][i:i+15]
    x19 = years[18][i:i+15]
    x20 = years[19][i:i+15]
    x21 = years[20][i:i+15]
    x22 = years[21][i:i+15]
    x23 = years[22][i:i+15]
    x24 = years[23][i:i+15]
    x25 = years[24][i:i+15]
    x26 = years[25][i:i+15]
    x27 = years[26][i:i+15]
    x28 = years[27][i:i+15]
    x29 = years[28][i:i+15]
    x30 = years[29][i:i+15]
    allvals = [x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,x11,x12,x13,x14,x15,x16,x17,x18,x19,x20,
               x21,x22,x23,x24,x25,x26,x27,x28,x29,x30]
    allvals2 = []
    for j in range(30):
        allvals2.extend(allvals[j])  
    daily_std[i] = np.std(allvals2)

std_fill1 = np.zeros(7)
std_fill2 = np.zeros(7)
daily_std = np.hstack([std_fill1,daily_std,std_fill2])
daily_std[:7] = 1000 # changed from 1 to 1000
daily_std[-7:] = 1000 # changed from 1 to 1000

np.save('daily_std_tmax2',daily_std)

# redefine vec for 1940-2023
vec = np.nanmean(tmax_normed2,axis=1)

tmax_anoms_z = np.empty(30660)
cell = vec
averages = daily_avg
stds = daily_std
all_avg = np.tile(averages,84)
all_std = np.tile(stds,84)
tmax_anoms_z = (cell - all_avg)/all_std

# sanity check - check to make sure z-scores look right
dates = pd.date_range(start='1/1/1940', end='12/31/2023')
dates = dates[~((dates.month == 2) & (dates.day == 29))]
dates = pd.DataFrame(dates)
dates['z_score'] = tmax_anoms_z

tmax_anoms_z = tmax_anoms_z.astype('float32')
np.save('tmax_anoms_z2.npy',tmax_anoms_z)
tmax_anoms_z = np.load('tmax_anoms_z2.npy') # this contains spurious values for first 7 and last 7 days of each year (actual anomalies)

dates['month'] = pd.to_datetime(dates.iloc[:,0]).dt.month
dates = dates[dates.month.isin([4,5,6,7,8,9])]
dates.drop(columns='month',inplace=True)
dates.reset_index(inplace=True)

data = np.array(dates.z_score)

np.save('tmax_z_normed2',data)

# making .nc files for Xarray

data = np.load('tmax_z_normed2.npy') 

ds = xr.DataArray(data, name='tmax_z', coords={'time':dates.iloc[:,1]},
                  dims=['time'])

ds.to_netcdf('tmax_z_normed2.nc')
